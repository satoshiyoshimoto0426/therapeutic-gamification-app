#!/usr/bin/env python3
"""
Task 9.1: [UNICODE_30B3] - [UNICODE_7C21]
OpenAI Moderation API[UNICODE_7D71]
"""

import sys
import os
import asyncio

# Add the services directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

def test_content_moderation_basic():
    """[UNICODE_30B3]"""
    print("=== Task 9.1: [UNICODE_30B3] ===")
    
    try:
        from main import (
            ContentModerationEngine,
            SafetyAnalysisRequest,
            SafetyThreatLevel
        )
        
        # [UNICODE_30A8]
        engine = ContentModerationEngine()
        print("[UNICODE_2705] ContentModerationEngine[UNICODE_521D]")
        
        # [UNICODE_81EA]
        patterns = engine.self_harm_patterns
        assert len(patterns) >= 5, "[UNICODE_81EA]"
        print(f"[UNICODE_2705] [UNICODE_81EA]: {len(patterns)}[UNICODE_500B]")
        
        # [UNICODE_6CBB]
        keywords = engine.therapeutic_keywords
        assert len(keywords) >= 10, "[UNICODE_6CBB]"
        print(f"[UNICODE_2705] [UNICODE_6CBB]: {len(keywords)}[UNICODE_500B]")
        
        # [UNICODE_5B89]
        safe_content = "[UNICODE_4ECA]"
        risk_score = engine._calculate_custom_risk_score(safe_content)
        assert risk_score < 0.1, f"[UNICODE_5B89]: {risk_score}"
        print(f"[UNICODE_2705] [UNICODE_5B89]: [UNICODE_30EA] {risk_score:.3f}")
        
        # [UNICODE_30EA]
        risky_content = "[UNICODE_3082]"
        risk_score = engine._calculate_custom_risk_score(risky_content)
        assert risk_score > 0.3, f"[UNICODE_30EA]: {risk_score}"
        print(f"[UNICODE_2705] [UNICODE_30EA]: [UNICODE_30EA] {risk_score:.3f}")
        
        # [UNICODE_6CBB]
        base_content = "[UNICODE_4FA1]"
        therapeutic_content = "[UNICODE_4FA1]"
        
        base_score = engine._calculate_custom_risk_score(base_content)
        therapeutic_score = engine._calculate_custom_risk_score(therapeutic_content)
        
        assert therapeutic_score < base_score, "[UNICODE_6CBB]"
        print(f"[UNICODE_2705] [UNICODE_6CBB]: {base_score:.3f} [UNICODE_2192] {therapeutic_score:.3f}")
        
        # F1[UNICODE_30B9]
        test_cases = [
            {"content": "[UNICODE_4ECA]", "expected_unsafe": False},
            {"content": "[UNICODE_6210]", "expected_unsafe": False},
            {"content": "[UNICODE_6B7B]", "expected_unsafe": True},
            {"content": "[UNICODE_81EA]", "expected_unsafe": True}
        ]
        
        performance = engine.validate_f1_performance(test_cases)
        assert "f1_score" in performance, "F1[UNICODE_30B9]"
        assert 0 <= performance["f1_score"] <= 1, "F1[UNICODE_30B9]"
        print(f"[UNICODE_2705] F1[UNICODE_30B9]: {performance['f1_score']:.3f} ([UNICODE_76EE]: 0.98)")
        
        print("\n[UNICODE_1F389] Task 9.1 [UNICODE_57FA]!")
        return True
        
    except Exception as e:
        print(f"[UNICODE_274C] [UNICODE_30C6]: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_openai_integration():
    """OpenAI Moderation API[UNICODE_7D71]"""
    print("\n=== OpenAI Moderation API[UNICODE_7D71] ===")
    
    try:
        from main import ContentModerationEngine
        
        engine = ContentModerationEngine()
        
        # [UNICODE_5B89]
        safe_result = await engine._check_openai_moderation("Hello, how are you?")
        assert "flagged" in safe_result, "OpenAI[UNICODE_7D50]flagged[UNICODE_30D5]"
        print(f"[UNICODE_2705] [UNICODE_5B89]: flagged={safe_result.get('flagged', 'unknown')}")
        
        # [UNICODE_30A8]API[UNICODE_30AD]
        error_result = await engine._check_openai_moderation("Test content")
        assert "flagged" in error_result, "[UNICODE_30A8]"
        print("[UNICODE_2705] [UNICODE_30A8]")
        
        print("[UNICODE_2705] OpenAI[UNICODE_7D71]")
        return True
        
    except Exception as e:
        print(f"[UNICODE_274C] OpenAI[UNICODE_7D71]: {e}")
        return False

async def test_safety_analysis_integration():
    """[UNICODE_5B89]"""
    print("\n=== [UNICODE_5B89] ===")
    
    try:
        from main import (
            ContentModerationEngine,
            SafetyAnalysisRequest
        )
        
        engine = ContentModerationEngine()
        
        # [UNICODE_5B89]
        safe_request = SafetyAnalysisRequest(
            uid="test_user",
            content="[UNICODE_4ECA]",
            content_type="user_input",
            user_context={"recent_mood": 4}
        )
        
        safe_result = await engine.analyze_content_safety(safe_request)
        assert safe_result.uid == "test_user", "[UNICODE_30E6]ID[UNICODE_304C]"
        assert safe_result.content_safe == True, "[UNICODE_5B89]"
        print("[UNICODE_2705] [UNICODE_5B89]")
        
        # [UNICODE_30EA]
        risky_request = SafetyAnalysisRequest(
            uid="test_user",
            content="[UNICODE_3082]",
            content_type="user_input",
            user_context={"recent_mood": 1}
        )
        
        risky_result = await engine.analyze_content_safety(risky_request)
        assert risky_result.content_safe == False, "[UNICODE_5371]"
        assert risky_result.escalation_required == True, "[UNICODE_30A8]"
        print("[UNICODE_2705] [UNICODE_30EA]")
        
        print("[UNICODE_2705] [UNICODE_5B89]")
        return True
        
    except Exception as e:
        print(f"[UNICODE_274C] [UNICODE_5B89]: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """[UNICODE_30E1]"""
    print("Task 9.1: [UNICODE_30B3] - [UNICODE_7C21]")
    print("=" * 60)
    
    success_count = 0
    total_tests = 3
    
    # [UNICODE_57FA]
    if test_content_moderation_basic():
        success_count += 1
    
    # OpenAI[UNICODE_7D71]
    if asyncio.run(test_openai_integration()):
        success_count += 1
    
    # [UNICODE_5B89]
    if asyncio.run(test_safety_analysis_integration()):
        success_count += 1
    
    print(f"\n=== [UNICODE_30C6] ===")
    print(f"[UNICODE_6210]: {success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("[UNICODE_1F389] [UNICODE_3059]")
        print("\n[UNICODE_2705] Task 9.1 [UNICODE_5B9F]:")
        print("- OpenAI Moderation API[UNICODE_7D71]")
        print("- [UNICODE_30AB]")
        print("- 98% F1[UNICODE_30B9]")
        print("- [UNICODE_5B89]")
        return True
    else:
        print(f"[UNICODE_26A0]  {total_tests - success_count}[UNICODE_500B]")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)