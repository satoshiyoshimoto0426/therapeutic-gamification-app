"""
Task 24.2 [UNICODE_7D71]: [UNICODE_30D5]
"""
import pytest
import asyncio
from datetime import datetime, timedelta
from unittest.mock import Mock, patch
import json

# [UNICODE_30C6]
from feedback_analysis import (
    FeedbackAnalysisEngine, FeedbackCategory, FeedbackPriority,
    SentimentType, FeedbackItem
)
from main import AlphaPlaytestEngine, FeedbackType

class TestFeedbackAnalysisIntegration:
    """[UNICODE_30D5]"""
    
    @pytest.fixture
    def feedback_engine(self):
        """[UNICODE_30D5]"""
        return FeedbackAnalysisEngine()
    
    @pytest.fixture
    def playtest_engine(self):
        """[UNICODE_30D7]"""
        engine = AlphaPlaytestEngine()
        engine.max_test_users = 10
        return engine
    
    @pytest.mark.asyncio
    async def test_feedback_submission_and_analysis(self, feedback_engine):
        """[UNICODE_30D5]"""
        
        # 1. [UNICODE_30DD]
        positive_feedback = await feedback_engine.submit_feedback(
            user_id="user_001",
            category=FeedbackCategory.USABILITY,
            title="[UNICODE_4F7F]",
            content="[UNICODE_3053]",
            rating=5,
            device_info={"platform": "iOS", "version": "14.0"},
            app_version="1.0.0"
        )
        
        # 2. [UNICODE_57FA]
        assert positive_feedback.feedback_id is not None
        assert positive_feedback.user_id == "user_001"
        assert positive_feedback.category == FeedbackCategory.USABILITY
        assert positive_feedback.rating == 5
        assert positive_feedback.processed
        
        # 3. [UNICODE_611F]
        sentiment = positive_feedback.sentiment_analysis
        assert sentiment is not None
        assert sentiment.overall_score > 0  # [UNICODE_30DD]
        assert sentiment.sentiment_type in [SentimentType.POSITIVE, SentimentType.VERY_POSITIVE]
        assert len(sentiment.positive_keywords) > 0
        
        # 4. [UNICODE_30C6]
        assert "ui_navigation" in positive_feedback.themes
        assert len(positive_feedback.keywords) > 0
        
        # 5. [UNICODE_512A]
        assert positive_feedback.priority == FeedbackPriority.LOW  # [UNICODE_30DD]5[UNICODE_FF09]
    
    @pytest.mark.asyncio
    async def test_negative_feedback_analysis(self, feedback_engine):
        """[UNICODE_30CD]"""
        
        # 1. [UNICODE_30CD]
        negative_feedback = await feedback_engine.submit_feedback(
            user_id="user_002",
            category=FeedbackCategory.BUG_REPORT,
            title="[UNICODE_30A2]",
            content="[UNICODE_30A2]",
            rating=1
        )
        
        # 2. [UNICODE_611F]
        sentiment = negative_feedback.sentiment_analysis
        assert sentiment.overall_score < 0  # [UNICODE_30CD]
        assert sentiment.sentiment_type in [SentimentType.NEGATIVE, SentimentType.VERY_NEGATIVE]
        assert len(sentiment.negative_keywords) > 0
        
        # 3. [UNICODE_512A]
        assert negative_feedback.priority == FeedbackPriority.CRITICAL
        
        # 4. [UNICODE_30C6]
        assert "performance" in negative_feedback.themes or "bug_report" in negative_feedback.themes
    
    @pytest.mark.asyncio
    async def test_theme_analysis_generation(self, feedback_engine):
        """[UNICODE_30C6]"""
        
        # 1. [UNICODE_8907]
        feedback_data = [
            {
                "user_id": "user_001",
                "category": FeedbackCategory.USABILITY,
                "title": "[UNICODE_30CA]",
                "content": "[UNICODE_30E1]",
                "rating": 2
            },
            {
                "user_id": "user_002",
                "category": FeedbackCategory.USABILITY,
                "title": "UI[UNICODE_6539]",
                "content": "[UNICODE_30DC]",
                "rating": 3
            },
            {
                "user_id": "user_003",
                "category": FeedbackCategory.PERFORMANCE,
                "title": "[UNICODE_52D5]",
                "content": "[UNICODE_30A2]",
                "rating": 2
            },
            {
                "user_id": "user_004",
                "category": FeedbackCategory.THERAPEUTIC_EFFECT,
                "title": "[UNICODE_52B9]",
                "content": "[UNICODE_3053]",
                "rating": 5
            }
        ]
        
        for data in feedback_data:
            await feedback_engine.submit_feedback(**data)
        
        # 2. [UNICODE_30C6]
        theme_analyses = await feedback_engine.generate_theme_analysis(days=30)
        
        # 3. [UNICODE_7D50]
        assert len(theme_analyses) > 0
        
        # UI/[UNICODE_30CA]
        ui_theme = next((t for t in theme_analyses if t.theme == "ui_navigation"), None)
        assert ui_theme is not None
        assert ui_theme.frequency >= 2  # 2[UNICODE_4EF6]
        assert ui_theme.user_count >= 2  # 2[UNICODE_4EBA]
        assert ui_theme.sentiment_score < 0  # [UNICODE_30CD]
        
        # [UNICODE_30D1]
        perf_theme = next((t for t in theme_analyses if t.theme == "performance"), None)
        assert perf_theme is not None
        assert perf_theme.frequency >= 1
        
        # [UNICODE_6CBB]
        therapeutic_theme = next((t for t in theme_analyses if t.theme == "therapeutic_effectiveness"), None)
        assert therapeutic_theme is not None
        assert therapeutic_theme.sentiment_score > 0  # [UNICODE_30DD]
    
    @pytest.mark.asyncio
    async def test_improvement_suggestions_generation(self, feedback_engine):
        """[UNICODE_6539]"""
        
        # 1. [UNICODE_554F]
        problem_feedback = [
            {
                "user_id": f"user_{i}",
                "category": FeedbackCategory.USABILITY,
                "title": "[UNICODE_30CA]",
                "content": "[UNICODE_30E1]",
                "rating": 2
            }
            for i in range(5)  # 5[UNICODE_4EBA]
        ]
        
        for feedback in problem_feedback:
            await feedback_engine.submit_feedback(**feedback)
        
        # 2. [UNICODE_30C6]
        theme_analyses = await feedback_engine.generate_theme_analysis(days=30)
        
        # 3. [UNICODE_6539]
        suggestions = await feedback_engine.generate_improvement_suggestions(theme_analyses)
        
        # 4. [UNICODE_7D50]
        assert len(suggestions) > 0
        
        # UI/[UNICODE_30CA]
        ui_suggestions = [s for s in suggestions if s["theme"] == "ui_navigation"]
        assert len(ui_suggestions) > 0
        
        ui_suggestion = ui_suggestions[0]
        assert ui_suggestion["urgency"] in ["[UNICODE_4E2D]", "[UNICODE_9AD8]"]  # [UNICODE_30CD]
        assert ui_suggestion["affected_users"] == 5
        assert ui_suggestion["frequency"] >= 5
        assert "[UNICODE_30CA]" in ui_suggestion["suggestion"]
        assert ui_suggestion["sentiment_score"] < 0
    
    @pytest.mark.asyncio
    async def test_feedback_statistics_calculation(self, feedback_engine):
        """[UNICODE_30D5]"""
        
        # 1. [UNICODE_591A]
        feedback_samples = [
            # [UNICODE_30D0]
            {
                "category": FeedbackCategory.BUG_REPORT,
                "title": "[UNICODE_30A8]",
                "content": "[UNICODE_30A8]",
                "rating": 1
            },
            # [UNICODE_6A5F]
            {
                "category": FeedbackCategory.FEATURE_REQUEST,
                "title": "[UNICODE_65B0]",
                "content": "[UNICODE_3053]",
                "rating": 4
            },
            # [UNICODE_30E6]
            {
                "category": FeedbackCategory.USABILITY,
                "title": "[UNICODE_4F7F]",
                "content": "[UNICODE_3082]",
                "rating": 3
            },
            # [UNICODE_6CBB]
            {
                "category": FeedbackCategory.THERAPEUTIC_EFFECT,
                "title": "[UNICODE_52B9]",
                "content": "[UNICODE_3068]",
                "rating": 5
            }
        ]
        
        for i, feedback_data in enumerate(feedback_samples):
            await feedback_engine.submit_feedback(
                user_id=f"stats_user_{i}",
                **feedback_data
            )
        
        # 2. [UNICODE_7D71]
        stats = feedback_engine.get_feedback_statistics(days=30)
        
        # 3. [UNICODE_7D50]
        assert stats["total_feedback"] == 4
        assert stats["unique_users"] == 4
        
        statistics = stats["statistics"]
        
        # [UNICODE_30AB]
        category_dist = statistics["category_distribution"]
        assert category_dist["bug_report"] == 1
        assert category_dist["feature_request"] == 1
        assert category_dist["usability"] == 1
        assert category_dist["therapeutic_effect"] == 1
        
        # [UNICODE_512A]
        priority_dist = statistics["priority_distribution"]
        assert priority_dist["high"] >= 1  # [UNICODE_30D0]
        assert priority_dist["medium"] >= 1  # [UNICODE_6A5F]
        assert priority_dist["low"] >= 1   # [UNICODE_6CBB]5[UNICODE_FF09]
        
        # [UNICODE_8A55]
        rating_dist = statistics["rating_distribution"]
        assert rating_dist[1] == 1  # 1[UNICODE_3064]
        assert rating_dist[3] == 1  # 3[UNICODE_3064]
        assert rating_dist[4] == 1  # 4[UNICODE_3064]
        assert rating_dist[5] == 1  # 5[UNICODE_3064]
        
        # [UNICODE_5E73]
        assert statistics["average_rating"] == 3.25  # (1+3+4+5)/4
        
        # [UNICODE_51E6]
        assert statistics["processing_rate"] == 1.0  # [UNICODE_5168]
    
    @pytest.mark.asyncio
    async def test_feedback_status_management(self, feedback_engine):
        """[UNICODE_30D5]"""
        
        # 1. [UNICODE_30D5]
        feedback = await feedback_engine.submit_feedback(
            user_id="status_user",
            category=FeedbackCategory.BUG_REPORT,
            title="[UNICODE_30D0]",
            content="[UNICODE_30D0]",
            rating=2
        )
        
        # 2. [UNICODE_521D]
        assert feedback.status == "open"
        assert feedback.assigned_to is None
        assert feedback.resolution is None
        
        # 3. [UNICODE_72B6]
        updated_feedback = await feedback_engine.update_feedback_status(
            feedback.feedback_id,
            status="in_progress",
            assigned_to="developer_001"
        )
        
        assert updated_feedback is not None
        assert updated_feedback.status == "in_progress"
        assert updated_feedback.assigned_to == "developer_001"
        
        # 4. [UNICODE_72B6]
        resolved_feedback = await feedback_engine.update_feedback_status(
            feedback.feedback_id,
            status="resolved",
            resolution="[UNICODE_30D0]"
        )
        
        assert resolved_feedback.status == "resolved"
        assert resolved_feedback.resolution is not None
        assert "[UNICODE_4FEE]" in resolved_feedback.resolution
    
    @pytest.mark.asyncio
    async def test_high_priority_feedback_filtering(self, feedback_engine):
        """[UNICODE_9AD8]"""
        
        # 1. [UNICODE_69D8]
        feedback_data = [
            # [UNICODE_30AF]
            {
                "category": FeedbackCategory.BUG_REPORT,
                "title": "[UNICODE_30AF]",
                "content": "[UNICODE_30A2]",
                "rating": 1
            },
            # [UNICODE_9AD8]
            {
                "category": FeedbackCategory.BUG_REPORT,
                "title": "[UNICODE_30A8]",
                "content": "[UNICODE_30A8]",
                "rating": 2
            },
            # [UNICODE_4E2D]
            {
                "category": FeedbackCategory.USABILITY,
                "title": "[UNICODE_6539]",
                "content": "[UNICODE_4F7F]",
                "rating": 3
            },
            # [UNICODE_4F4E]
            {
                "category": FeedbackCategory.GENERAL,
                "title": "[UNICODE_611F]",
                "content": "[UNICODE_826F]",
                "rating": 4
            }
        ]
        
        submitted_feedback = []
        for i, data in enumerate(feedback_data):
            feedback = await feedback_engine.submit_feedback(
                user_id=f"priority_user_{i}",
                **data
            )
            submitted_feedback.append(feedback)
        
        # 2. [UNICODE_9AD8]
        high_priority = feedback_engine.get_high_priority_feedback()
        
        # 3. [UNICODE_7D50]
        assert len(high_priority) >= 2  # [UNICODE_30AF]
        
        # [UNICODE_5168]
        for feedback in high_priority:
            assert feedback.priority in [FeedbackPriority.HIGH, FeedbackPriority.CRITICAL]
            assert feedback.status in ["open", "in_progress"]
        
        # [UNICODE_30AF]
        critical_feedback = [fb for fb in high_priority if fb.priority == FeedbackPriority.CRITICAL]
        assert len(critical_feedback) >= 1
    
    @pytest.mark.asyncio
    async def test_user_feedback_history(self, feedback_engine):
        """[UNICODE_30E6]"""
        
        # 1. [UNICODE_7279]
        user_id = "history_user"
        feedback_history = [
            {
                "category": FeedbackCategory.USABILITY,
                "title": "[UNICODE_521D]",
                "content": "[UNICODE_4F7F]",
                "rating": 3
            },
            {
                "category": FeedbackCategory.FEATURE_REQUEST,
                "title": "[UNICODE_6A5F]",
                "content": "[UNICODE_3053]",
                "rating": 4
            },
            {
                "category": FeedbackCategory.THERAPEUTIC_EFFECT,
                "title": "[UNICODE_52B9]",
                "content": "[UNICODE_6539]",
                "rating": 5
            }
        ]
        
        for feedback_data in feedback_history:
            await feedback_engine.submit_feedback(user_id=user_id, **feedback_data)
            await asyncio.sleep(0.01)  # [UNICODE_6642]
        
        # 2. [UNICODE_30E6]
        user_feedback = feedback_engine.get_feedback_by_user(user_id)
        
        # 3. [UNICODE_7D50]
        assert len(user_feedback) == 3
        
        # [UNICODE_5168]
        for feedback in user_feedback:
            assert feedback.user_id == user_id
        
        # [UNICODE_6642]
        timestamps = [fb.created_at for fb in user_feedback]
        assert timestamps == sorted(timestamps)
        
        # [UNICODE_8A55]
        ratings = [fb.rating for fb in user_feedback]
        assert ratings == [3, 4, 5]  # [UNICODE_6539]
    
    @pytest.mark.asyncio
    async def test_feedback_export_report(self, feedback_engine):
        """[UNICODE_30D5]"""
        
        # 1. [UNICODE_30B5]
        sample_feedback = [
            {
                "user_id": "export_user_1",
                "category": FeedbackCategory.BUG_REPORT,
                "title": "[UNICODE_30D0]",
                "content": "[UNICODE_30D0]",
                "rating": 2
            },
            {
                "user_id": "export_user_2",
                "category": FeedbackCategory.USABILITY,
                "title": "UI[UNICODE_6539]",
                "content": "UI[UNICODE_3092]",
                "rating": 3
            }
        ]
        
        for feedback_data in sample_feedback:
            await feedback_engine.submit_feedback(**feedback_data)
        
        # 2. [UNICODE_30EC]
        report_json = feedback_engine.export_feedback_report(format="json")
        
        # 3. [UNICODE_7D50]
        assert report_json is not None
        
        # JSON[UNICODE_89E3]
        report_data = json.loads(report_json)
        
        # [UNICODE_57FA]
        assert "generated_at" in report_data
        assert "summary" in report_data
        assert "theme_analysis" in report_data
        assert "high_priority_items" in report_data
        assert "recent_feedback" in report_data
        
        # [UNICODE_30B5]
        summary = report_data["summary"]
        assert summary["total_feedback"] >= 2
        assert summary["unique_users"] >= 2
        
        # [UNICODE_6700]
        recent_feedback = report_data["recent_feedback"]
        assert len(recent_feedback) >= 2
        
        for feedback in recent_feedback:
            assert "feedback_id" in feedback
            assert "category" in feedback
            assert "title" in feedback
            assert "rating" in feedback
            assert "priority" in feedback
            assert "status" in feedback
            assert "created_at" in feedback

if __name__ == "__main__":
    pytest.main([__file__, "-v"])