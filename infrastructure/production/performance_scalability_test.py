#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import json
import logging
import sys
import time
import statistics
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import aiohttp
import concurrent.futures
import psutil
import requests

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceScalabilityTester:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "target": base_url,
            "tests": {},
            "summary": {},
            "overall_status": "UNKNOWN"
        }
    
    async def test_response_time(self) -> Dict:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ†ã‚¹ãƒˆ"""
        logger.info("ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        test_result = {
            "name": "response_time",
            "status": "UNKNOWN",
            "metrics": {},
            "details": {}
        }
        
        try:
            # è¤‡æ•°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š
            endpoints = [
                "/health",
                "/api/auth/health",
                "/api/game/health",
                "/api/tasks/health",
                "/api/mandala/health"
            ]
            
            endpoint_results = {}
            
            for endpoint in endpoints:
                response_times = []
                
                # å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’10å›æ¸¬å®š
                for _ in range(10):
                    start_time = time.time()
                    try:
                        response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                        end_time = time.time()
                        
                        if response.status_code == 200:
                            response_times.append((end_time - start_time) * 1000)  # ãƒŸãƒªç§’
                    except Exception:
                        pass
                
                if response_times:
                    endpoint_results[endpoint] = {
                        "samples": len(response_times),
                        "min_ms": min(response_times),
                        "max_ms": max(response_times),
                        "avg_ms": statistics.mean(response_times),
                        "median_ms": statistics.median(response_times),
                        "p95_ms": sorted(response_times)[int(len(response_times) * 0.95)],
                        "p99_ms": sorted(response_times)[int(len(response_times) * 0.99)]
                    }
            
            test_result["details"]["endpoints"] = endpoint_results
            
            # å…¨ä½“çµ±è¨ˆ
            all_response_times = []
            for endpoint_data in endpoint_results.values():
                # å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¹³å‡å€¤ã‚’ä½¿ç”¨
                all_response_times.append(endpoint_data["avg_ms"])
            
            if all_response_times:
                test_result["metrics"] = {
                    "overall_avg_ms": statistics.mean(all_response_times),
                    "overall_p95_ms": sorted(all_response_times)[int(len(all_response_times) * 0.95)],
                    "target_p95_ms": 1200,  # 1.2ç§’ç›®æ¨™
                    "meets_target": sorted(all_response_times)[int(len(all_response_times) * 0.95)] <= 1200
                }
                
                if test_result["metrics"]["meets_target"]:
                    test_result["status"] = "PASS"
                else:
                    test_result["status"] = "FAIL"
            else:
                test_result["status"] = "ERROR"
                
        except Exception as e:
            test_result["status"] = "ERROR"
            test_result["error"] = str(e)
        
        return test_result
    
    async def test_throughput(self) -> Dict:
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        test_result = {
            "name": "throughput",
            "status": "UNKNOWN",
            "metrics": {},
            "details": {}
        }
        
        try:
            # æ®µéšçš„è² è·ãƒ†ã‚¹ãƒˆ
            load_levels = [10, 25, 50, 100, 200]
            throughput_results = {}
            
            for concurrent_users in load_levels:
                logger.info(f"åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•° {concurrent_users} ã§ãƒ†ã‚¹ãƒˆä¸­...")
                
                async with aiohttp.ClientSession() as session:
                    start_time = time.time()
                    
                    # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
                    tasks = []
                    for _ in range(concurrent_users):
                        task = self.make_async_request(session, f"{self.base_url}/health")
                        tasks.append(task)
                    
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    end_time = time.time()
                    
                    # çµæœåˆ†æ
                    successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
                    total_time = end_time - start_time
                    
                    throughput_results[concurrent_users] = {
                        "concurrent_users": concurrent_users,
                        "total_requests": len(results),
                        "successful_requests": successful_requests,
                        "failed_requests": len(results) - successful_requests,
                        "success_rate": successful_requests / len(results),
                        "total_time_seconds": total_time,
                        "requests_per_second": successful_requests / total_time if total_time > 0 else 0,
                        "avg_response_time_ms": statistics.mean([r.get("response_time", 0) for r in results if isinstance(r, dict) and r.get("response_time")])
                    }
                    
                    # æˆåŠŸç‡ãŒ90%ã‚’ä¸‹å›ã£ãŸã‚‰åœæ­¢
                    if successful_requests / len(results) < 0.9:
                        logger.warning(f"æˆåŠŸç‡ãŒ90%ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ: {successful_requests / len(results):.1%}")
                        break
            
            test_result["details"]["load_levels"] = throughput_results
            
            # æœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
            max_rps = max(data["requests_per_second"] for data in throughput_results.values())
            target_rps = 120  # 120 req/min = 2 req/sec
            
            test_result["metrics"] = {
                "max_requests_per_second": max_rps,
                "target_requests_per_second": target_rps,
                "meets_target": max_rps >= target_rps,
                "max_concurrent_users": max(throughput_results.keys())
            }
            
            if test_result["metrics"]["meets_target"]:
                test_result["status"] = "PASS"
            else:
                test_result["status"] = "FAIL"
                
        except Exception as e:
            test_result["status"] = "ERROR"
            test_result["error"] = str(e)
        
        return test_result
    
    async def make_async_request(self, session: aiohttp.ClientSession, url: str) -> Dict:
        """éåŒæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            start_time = time.time()
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                end_time = time.time()
                
                return {
                    "success": response.status == 200,
                    "status_code": response.status,
                    "response_time": (end_time - start_time) * 1000  # ãƒŸãƒªç§’
                }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": 0
            }  
  
    async def test_scalability(self) -> Dict:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        logger.info("ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        test_result = {
            "name": "scalability",
            "status": "UNKNOWN",
            "metrics": {},
            "details": {}
        }
        
        try:
            # æ®µéšçš„è² è·å¢—åŠ ãƒ†ã‚¹ãƒˆ
            scalability_phases = [
                {"users": 100, "duration": 30},   # 100ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€30ç§’
                {"users": 500, "duration": 60},   # 500ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€60ç§’
                {"users": 1000, "duration": 60},  # 1000ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€60ç§’
                {"users": 2000, "duration": 30}   # 2000ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€30ç§’
            ]
            
            phase_results = {}
            
            for i, phase in enumerate(scalability_phases):
                logger.info(f"ãƒ•ã‚§ãƒ¼ã‚º {i+1}: {phase['users']}ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€{phase['duration']}ç§’")
                
                # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
                initial_cpu = psutil.cpu_percent(interval=1)
                initial_memory = psutil.virtual_memory().percent
                
                # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                start_time = time.time()
                
                async with aiohttp.ClientSession() as session:
                    # æŒ‡å®šæ™‚é–“å†…ã§ç¶™ç¶šçš„ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
                    tasks = []
                    end_time = start_time + phase["duration"]
                    
                    while time.time() < end_time:
                        # åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°åˆ†ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                        batch_tasks = []
                        for _ in range(min(phase["users"], 50)):  # ä¸€åº¦ã«æœ€å¤§50ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                            task = self.make_async_request(session, f"{self.base_url}/health")
                            batch_tasks.append(task)
                        
                        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                        tasks.extend(batch_results)
                        
                        # çŸ­æ™‚é–“å¾…æ©Ÿ
                        await asyncio.sleep(0.1)
                
                # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–çµ‚äº†
                final_cpu = psutil.cpu_percent(interval=1)
                final_memory = psutil.virtual_memory().percent
                
                # çµæœåˆ†æ
                successful_requests = sum(1 for r in tasks if isinstance(r, dict) and r.get("success", False))
                total_requests = len(tasks)
                actual_duration = time.time() - start_time
                
                phase_results[f"phase_{i+1}"] = {
                    "target_users": phase["users"],
                    "duration_seconds": actual_duration,
                    "total_requests": total_requests,
                    "successful_requests": successful_requests,
                    "success_rate": successful_requests / total_requests if total_requests > 0 else 0,
                    "requests_per_second": successful_requests / actual_duration if actual_duration > 0 else 0,
                    "system_resources": {
                        "cpu_before": initial_cpu,
                        "cpu_after": final_cpu,
                        "cpu_increase": final_cpu - initial_cpu,
                        "memory_before": initial_memory,
                        "memory_after": final_memory,
                        "memory_increase": final_memory - initial_memory
                    }
                }
                
                # æˆåŠŸç‡ãŒ80%ã‚’ä¸‹å›ã£ãŸã‚‰åœæ­¢
                if successful_requests / total_requests < 0.8:
                    logger.warning(f"æˆåŠŸç‡ãŒ80%ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ: {successful_requests / total_requests:.1%}")
                    break
                
                # ãƒ•ã‚§ãƒ¼ã‚ºé–“ã®ä¼‘æ†©
                await asyncio.sleep(5)
            
            test_result["details"]["phases"] = phase_results
            
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è©•ä¾¡
            max_successful_users = 0
            for phase_name, phase_data in phase_results.items():
                if phase_data["success_rate"] >= 0.9:  # 90%ä»¥ä¸Šã®æˆåŠŸç‡
                    max_successful_users = max(max_successful_users, phase_data["target_users"])
            
            target_users = 20000  # 20,000åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ç›®æ¨™
            
            test_result["metrics"] = {
                "max_successful_concurrent_users": max_successful_users,
                "target_concurrent_users": target_users,
                "scalability_ratio": max_successful_users / target_users,
                "meets_target": max_successful_users >= target_users * 0.1  # 10%ã§ã‚‚éƒ¨åˆ†çš„æˆåŠŸ
            }
            
            if max_successful_users >= target_users:
                test_result["status"] = "PASS"
            elif max_successful_users >= target_users * 0.1:
                test_result["status"] = "PARTIAL"
            else:
                test_result["status"] = "FAIL"
                
        except Exception as e:
            test_result["status"] = "ERROR"
            test_result["error"] = str(e)
        
        return test_result
    
    async def test_resource_usage(self) -> Dict:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ"""
        logger.info("ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        test_result = {
            "name": "resource_usage",
            "status": "UNKNOWN",
            "metrics": {},
            "details": {}
        }
        
        try:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
            baseline_cpu = psutil.cpu_percent(interval=1)
            baseline_memory = psutil.virtual_memory().percent
            baseline_disk = psutil.disk_usage('/').percent
            
            # è² è·ã‚’ã‹ã‘ãªãŒã‚‰ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
            monitoring_duration = 60  # 60ç§’é–“ç›£è¦–
            monitoring_interval = 5   # 5ç§’é–“éš”
            
            resource_samples = []
            
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                
                # ç¶™ç¶šçš„ãªè² è·ç”Ÿæˆ
                load_task = asyncio.create_task(self.generate_continuous_load(session))
                
                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
                while time.time() - start_time < monitoring_duration:
                    sample_time = time.time()
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory_info = psutil.virtual_memory()
                    disk_info = psutil.disk_usage('/')
                    
                    resource_samples.append({
                        "timestamp": sample_time - start_time,
                        "cpu_percent": cpu_percent,
                        "memory_percent": memory_info.percent,
                        "memory_used_gb": memory_info.used / (1024**3),
                        "disk_percent": disk_info.percent
                    })
                    
                    await asyncio.sleep(monitoring_interval)
                
                # è² è·ç”Ÿæˆåœæ­¢
                load_task.cancel()
                try:
                    await load_task
                except asyncio.CancelledError:
                    pass
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡åˆ†æ
            if resource_samples:
                cpu_values = [s["cpu_percent"] for s in resource_samples]
                memory_values = [s["memory_percent"] for s in resource_samples]
                
                test_result["details"]["samples"] = resource_samples
                test_result["metrics"] = {
                    "baseline": {
                        "cpu_percent": baseline_cpu,
                        "memory_percent": baseline_memory,
                        "disk_percent": baseline_disk
                    },
                    "under_load": {
                        "cpu_avg": statistics.mean(cpu_values),
                        "cpu_max": max(cpu_values),
                        "memory_avg": statistics.mean(memory_values),
                        "memory_max": max(memory_values)
                    },
                    "thresholds": {
                        "cpu_threshold": 80,
                        "memory_threshold": 80
                    }
                }
                
                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                cpu_ok = max(cpu_values) <= 80
                memory_ok = max(memory_values) <= 80
                
                if cpu_ok and memory_ok:
                    test_result["status"] = "PASS"
                elif cpu_ok or memory_ok:
                    test_result["status"] = "PARTIAL"
                else:
                    test_result["status"] = "FAIL"
            else:
                test_result["status"] = "ERROR"
                test_result["error"] = "No resource samples collected"
                
        except Exception as e:
            test_result["status"] = "ERROR"
            test_result["error"] = str(e)
        
        return test_result
    
    async def generate_continuous_load(self, session: aiohttp.ClientSession):
        """ç¶™ç¶šçš„ãªè² è·ç”Ÿæˆ"""
        try:
            while True:
                # è¤‡æ•°ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                tasks = []
                endpoints = ["/health", "/api/auth/health", "/api/game/health"]
                
                for endpoint in endpoints:
                    for _ in range(5):  # å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«5ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                        task = self.make_async_request(session, f"{self.base_url}{endpoint}")
                        tasks.append(task)
                
                await asyncio.gather(*tasks, return_exceptions=True)
                await asyncio.sleep(0.5)  # 0.5ç§’é–“éš”
                
        except asyncio.CancelledError:
            pass
    
    async def test_memory_leaks(self) -> Dict:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        logger.info("ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        test_result = {
            "name": "memory_leaks",
            "status": "UNKNOWN",
            "metrics": {},
            "details": {}
        }
        
        try:
            # é•·æ™‚é–“ã®ç¶™ç¶šçš„è² è·ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤‰åŒ–ã‚’ç›£è¦–
            test_duration = 300  # 5åˆ†é–“
            sample_interval = 30  # 30ç§’é–“éš”
            
            memory_samples = []
            
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                
                # ç¶™ç¶šçš„è² è·ç”Ÿæˆé–‹å§‹
                load_task = asyncio.create_task(self.generate_continuous_load(session))
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
                while time.time() - start_time < test_duration:
                    elapsed_time = time.time() - start_time
                    memory_info = psutil.virtual_memory()
                    
                    memory_samples.append({
                        "elapsed_seconds": elapsed_time,
                        "memory_used_gb": memory_info.used / (1024**3),
                        "memory_percent": memory_info.percent
                    })
                    
                    logger.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_info.percent:.1f}% ({memory_info.used / (1024**3):.2f}GB)")
                    
                    await asyncio.sleep(sample_interval)
                
                # è² è·ç”Ÿæˆåœæ­¢
                load_task.cancel()
                try:
                    await load_task
                except asyncio.CancelledError:
                    pass
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ†æ
            if len(memory_samples) >= 3:
                initial_memory = memory_samples[0]["memory_used_gb"]
                final_memory = memory_samples[-1]["memory_used_gb"]
                memory_increase = final_memory - initial_memory
                
                # ç·šå½¢å›å¸°ã§ãƒ¡ãƒ¢ãƒªå¢—åŠ å‚¾å‘ã‚’åˆ†æ
                x_values = [s["elapsed_seconds"] for s in memory_samples]
                y_values = [s["memory_used_gb"] for s in memory_samples]
                
                # ç°¡æ˜“ç·šå½¢å›å¸°
                n = len(x_values)
                sum_x = sum(x_values)
                sum_y = sum(y_values)
                sum_xy = sum(x * y for x, y in zip(x_values, y_values))
                sum_x2 = sum(x * x for x in x_values)
                
                slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
                
                test_result["details"]["samples"] = memory_samples
                test_result["metrics"] = {
                    "initial_memory_gb": initial_memory,
                    "final_memory_gb": final_memory,
                    "memory_increase_gb": memory_increase,
                    "memory_increase_rate_gb_per_hour": slope * 3600,  # 1æ™‚é–“ã‚ãŸã‚Šã®å¢—åŠ ç‡
                    "test_duration_minutes": test_duration / 60
                }
                
                # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ¤å®š
                # 1æ™‚é–“ã‚ãŸã‚Š100MBä»¥ä¸Šã®å¢—åŠ ã‚’ãƒªãƒ¼ã‚¯ã¨ã¿ãªã™
                leak_threshold = 0.1  # 0.1GB = 100MB
                
                if abs(slope * 3600) <= leak_threshold:
                    test_result["status"] = "PASS"
                elif abs(slope * 3600) <= leak_threshold * 2:
                    test_result["status"] = "PARTIAL"
                else:
                    test_result["status"] = "FAIL"
            else:
                test_result["status"] = "ERROR"
                test_result["error"] = "Insufficient memory samples"
                
        except Exception as e:
            test_result["status"] = "ERROR"
            test_result["error"] = str(e)
        
        return test_result   
 
    async def run_all_tests(self) -> Dict:
        """å…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # å„ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œï¼ˆãƒªã‚½ãƒ¼ã‚¹ã®ç«¶åˆã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        tests = [
            self.test_response_time(),
            self.test_throughput(),
            self.test_scalability(),
            self.test_resource_usage(),
            self.test_memory_leaks()
        ]
        
        for test_coro in tests:
            try:
                result = await test_coro
                self.test_results["tests"][result["name"]] = result
                
                # ãƒ†ã‚¹ãƒˆé–“ã®ä¼‘æ†©
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                self.test_results["tests"]["error"] = {
                    "name": "execution_error",
                    "status": "ERROR",
                    "error": str(e)
                }
        
        # ã‚µãƒãƒªãƒ¼ä½œæˆ
        self.create_summary()
        
        return self.test_results
    
    def create_summary(self):
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ä½œæˆ"""
        total_tests = len(self.test_results["tests"])
        passed_tests = sum(1 for t in self.test_results["tests"].values() if t.get("status") == "PASS")
        partial_tests = sum(1 for t in self.test_results["tests"].values() if t.get("status") == "PARTIAL")
        failed_tests = sum(1 for t in self.test_results["tests"].values() if t.get("status") == "FAIL")
        error_tests = sum(1 for t in self.test_results["tests"].values() if t.get("status") == "ERROR")
        
        self.test_results["summary"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "partial_tests": partial_tests,
            "failed_tests": failed_tests,
            "error_tests": error_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
            "completion_rate": (passed_tests + partial_tests) / total_tests if total_tests > 0 else 0
        }
        
        # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
        if failed_tests == 0 and error_tests == 0:
            if passed_tests == total_tests:
                self.test_results["overall_status"] = "PASS"
            else:
                self.test_results["overall_status"] = "PARTIAL"
        else:
            self.test_results["overall_status"] = "FAIL"
    
    def generate_report(self) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ

## æ¦‚è¦
- **å®Ÿè¡Œæ—¥æ™‚**: {self.test_results['timestamp']}
- **ãƒ†ã‚¹ãƒˆå¯¾è±¡**: {self.test_results['target']}
- **å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {self.test_results['overall_status']}

## ã‚µãƒãƒªãƒ¼
- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {self.test_results['summary']['total_tests']}
- **æˆåŠŸ**: {self.test_results['summary']['passed_tests']}
- **éƒ¨åˆ†æˆåŠŸ**: {self.test_results['summary']['partial_tests']}
- **å¤±æ•—**: {self.test_results['summary']['failed_tests']}
- **ã‚¨ãƒ©ãƒ¼**: {self.test_results['summary']['error_tests']}
- **æˆåŠŸç‡**: {self.test_results['summary']['success_rate']:.1%}

## ãƒ†ã‚¹ãƒˆçµæœè©³ç´°

"""
        
        for test_name, test_result in self.test_results["tests"].items():
            status_icon = {
                "PASS": "âœ…",
                "PARTIAL": "âš ï¸",
                "FAIL": "âŒ",
                "ERROR": "ğŸ’¥",
                "UNKNOWN": "â“"
            }.get(test_result.get("status", "UNKNOWN"), "â“")
            
            report += f"### {status_icon} {test_result.get('name', test_name).upper()}\n"
            report += f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {test_result.get('status', 'UNKNOWN')}\n\n"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            if test_result.get("metrics"):
                report += "**ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:\n"
                for key, value in test_result["metrics"].items():
                    if isinstance(value, dict):
                        report += f"- **{key}**:\n"
                        for sub_key, sub_value in value.items():
                            report += f"  - {sub_key}: {sub_value}\n"
                    else:
                        if isinstance(value, float):
                            report += f"- **{key}**: {value:.2f}\n"
                        else:
                            report += f"- **{key}**: {value}\n"
                report += "\n"
            
            # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
            if test_result.get("error"):
                report += f"**ã‚¨ãƒ©ãƒ¼**: {test_result['error']}\n\n"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã¨ã®æ¯”è¼ƒ
        report += "## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã¨ã®æ¯”è¼ƒ\n\n"
        
        response_time_test = self.test_results["tests"].get("response_time", {})
        if response_time_test.get("metrics"):
            p95_time = response_time_test["metrics"].get("overall_p95_ms", 0)
            target_time = response_time_test["metrics"].get("target_p95_ms", 1200)
            
            if p95_time <= target_time:
                report += f"- âœ… **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {p95_time:.1f}ms (ç›®æ¨™: {target_time}msä»¥ä¸‹)\n"
            else:
                report += f"- âŒ **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {p95_time:.1f}ms (ç›®æ¨™: {target_time}msä»¥ä¸‹)\n"
        
        throughput_test = self.test_results["tests"].get("throughput", {})
        if throughput_test.get("metrics"):
            max_rps = throughput_test["metrics"].get("max_requests_per_second", 0)
            target_rps = throughput_test["metrics"].get("target_requests_per_second", 2)
            
            if max_rps >= target_rps:
                report += f"- âœ… **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {max_rps:.1f} req/sec (ç›®æ¨™: {target_rps} req/secä»¥ä¸Š)\n"
            else:
                report += f"- âŒ **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {max_rps:.1f} req/sec (ç›®æ¨™: {target_rps} req/secä»¥ä¸Š)\n"
        
        scalability_test = self.test_results["tests"].get("scalability", {})
        if scalability_test.get("metrics"):
            max_users = scalability_test["metrics"].get("max_successful_concurrent_users", 0)
            target_users = scalability_test["metrics"].get("target_concurrent_users", 20000)
            
            if max_users >= target_users:
                report += f"- âœ… **åŒæ™‚æ¥ç¶šæ•°**: {max_users} users (ç›®æ¨™: {target_users} usersä»¥ä¸Š)\n"
            else:
                report += f"- âš ï¸ **åŒæ™‚æ¥ç¶šæ•°**: {max_users} users (ç›®æ¨™: {target_users} usersä»¥ä¸Š)\n"
        
        # æ¨å¥¨äº‹é …
        report += "\n## æ¨å¥¨äº‹é …\n\n"
        
        if self.test_results["overall_status"] == "PASS":
            report += "- âœ… ã™ã¹ã¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸã€‚\n"
            report += "- å®šæœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚\n"
        elif self.test_results["overall_status"] == "PARTIAL":
            report += "- âš ï¸ ä¸€éƒ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚\n"
            report += "- ä»¥ä¸‹ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ï¼š\n"
            report += "  - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–\n"
            report += "  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®è¦‹ç›´ã—\n"
            report += "  - ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã®èª¿æ•´\n"
        else:
            report += "- âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚\n"
            report += "- æœ¬ç•ªãƒªãƒªãƒ¼ã‚¹å‰ã«ä»¥ä¸‹ã®å¯¾å¿œãŒå¿…è¦ã§ã™ï¼š\n"
            report += "  - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–\n"
            report += "  - ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®å¼·åŒ–\n"
            report += "  - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®šã¨è§£æ±º\n"
        
        return report

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    parser.add_argument("--target", default="http://localhost:8080", help="ãƒ†ã‚¹ãƒˆå¯¾è±¡URL")
    parser.add_argument("--output", help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--format", choices=["json", "markdown"], default="markdown", help="å‡ºåŠ›å½¢å¼")
    parser.add_argument("--quick", action="store_true", help="ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆçŸ­æ™‚é–“ï¼‰")
    
    args = parser.parse_args()
    
    try:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        tester = PerformanceScalabilityTester(args.target)
        
        if args.quick:
            # ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ã¿ï¼‰
            logger.info("ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ...")
            response_time_result = await tester.test_response_time()
            throughput_result = await tester.test_throughput()
            
            tester.test_results["tests"]["response_time"] = response_time_result
            tester.test_results["tests"]["throughput"] = throughput_result
            tester.create_summary()
            
            results = tester.test_results
        else:
            # ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆ
            results = await tester.run_all_tests()
        
        # çµæœå‡ºåŠ›
        if args.format == "json":
            output_content = json.dumps(results, indent=2, ensure_ascii=False)
        else:
            output_content = tester.generate_report()
        
        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(output_content)
            logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {args.output}")
        else:
            print(output_content)
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
        if results["overall_status"] == "PASS":
            sys.exit(0)
        elif results["overall_status"] == "PARTIAL":
            sys.exit(1)
        else:
            sys.exit(2)
            
    except Exception as e:
        logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())