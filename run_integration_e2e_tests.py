#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ã‚¿ã‚¹ã‚¯27.2)

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œã—ã¾ã™:
1. åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
2. ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
3. ã‚¨ãƒ©ãƒ¼å‡¦ç†é©åˆ‡æ€§ç¢ºèªãƒ†ã‚¹ãƒˆ

è¦ä»¶:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼å…¨ä½“ã®å‹•ä½œç¢ºèª
- æœã®ã‚¿ã‚¹ã‚¯é…ä¿¡ã‹ã‚‰å¤œã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆã¾ã§ã®åŸºæœ¬ãƒ•ãƒ­ãƒ¼
- ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã®å‹•ä½œç¢ºèª
- ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®é©åˆ‡æ€§ç¢ºèª
- ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å‹•ä½œä¿è¨¼
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any
import logging
import subprocess
import sys

# ä½œæˆã—ãŸãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from test_integration_e2e_comprehensive import IntegrationE2ETester
from test_data_persistence_verification import DataPersistenceVerifier
from test_error_handling_comprehensive import ErrorHandlingTester

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IntegrationE2ETestRunner:
    """çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
    async def check_prerequisites(self) -> bool:
        """å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ” å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
        
        # å¿…è¦ãªã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        required_services = [
            ("èªè¨¼ã‚µãƒ¼ãƒ“ã‚¹", "http://localhost:8002"),
            ("ã‚³ã‚¢ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³", "http://localhost:8001"),
            ("ã‚¿ã‚¹ã‚¯ç®¡ç†", "http://localhost:8003"),
            ("Mandalaã‚·ã‚¹ãƒ†ãƒ ", "http://localhost:8004")
        ]
        
        import httpx
        
        missing_services = []
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            for service_name, url in required_services:
                try:
                    response = await client.get(f"{url}/health")
                    if response.status_code not in [200, 404]:  # 404ã§ã‚‚ã‚µãƒ¼ãƒ“ã‚¹ã¯å‹•ä½œä¸­
                        missing_services.append(service_name)
                except:
                    missing_services.append(service_name)
        
        if missing_services:
            logger.error(f"âŒ ä»¥ä¸‹ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“: {', '.join(missing_services)}")
            logger.error("deploy_local.pyã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return False
        
        logger.info("âœ… å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†")
        return True
    
    async def run_comprehensive_integration_tests(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        logger.info("="*80)
        
        tester = IntegrationE2ETester()
        
        try:
            results = await tester.run_comprehensive_tests()
            tester.print_comprehensive_summary(results)
            return results
        except Exception as e:
            logger.error(f"åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"error": str(e)}
    
    async def run_data_persistence_tests(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        logger.info("="*80)
        
        verifier = DataPersistenceVerifier()
        
        try:
            results = await verifier.run_persistence_tests()
            verifier.print_persistence_summary(results)
            return results
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"error": str(e)}
    
    async def run_error_handling_tests(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš¨ ã‚¨ãƒ©ãƒ¼å‡¦ç†é©åˆ‡æ€§ç¢ºèªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        logger.info("="*80)
        
        tester = ErrorHandlingTester()
        
        try:
            results = await tester.run_error_handling_tests()
            tester.print_error_handling_summary(results)
            return results
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"error": str(e)}
    
    def calculate_overall_success_metrics(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """å…¨ä½“çš„ãªæˆåŠŸæŒ‡æ¨™è¨ˆç®—"""
        metrics = {
            "total_test_categories": 0,
            "successful_categories": 0,
            "test_details": {},
            "overall_success_rate": 0.0,
            "critical_failures": [],
            "recommendations": []
        }
        
        # åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆçµæœåˆ†æ
        if "comprehensive_integration" in all_results:
            comp_results = all_results["comprehensive_integration"]
            metrics["total_test_categories"] += 1
            
            if not comp_results.get("error"):
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼æˆåŠŸç‡
                journey_results = comp_results.get("user_journey", {})
                if journey_results and not journey_results.get("error"):
                    journey_success_count = sum(1 for k, v in journey_results.items() 
                                              if isinstance(v, dict) and v.get("success", False))
                    journey_total = len([k for k in journey_results.keys() if k != "error"])
                    
                    if journey_total > 0:
                        journey_success_rate = journey_success_count / journey_total
                        metrics["test_details"]["user_journey_success_rate"] = journey_success_rate
                        
                        if journey_success_rate >= 0.8:
                            metrics["successful_categories"] += 1
                        else:
                            metrics["critical_failures"].append("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼ãƒ†ã‚¹ãƒˆã®æˆåŠŸç‡ãŒä½ã„")
                
                # ã‚¨ãƒ©ãƒ¼å‡¦ç†æˆåŠŸç‡
                error_handling = comp_results.get("error_handling", {})
                if error_handling:
                    error_rate = error_handling.get("error_handling_rate", 0)
                    metrics["test_details"]["error_handling_rate"] = error_rate
                    
                    if error_rate >= 0.7:
                        metrics["successful_categories"] += 0.5  # éƒ¨åˆ†çš„æˆåŠŸ
            else:
                metrics["critical_failures"].append("åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ããªã„")
        
        # ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆçµæœåˆ†æ
        if "data_persistence" in all_results:
            persist_results = all_results["data_persistence"]
            metrics["total_test_categories"] += 1
            
            if not persist_results.get("error"):
                # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æˆåŠŸç‡è¨ˆç®—
                persistence_success_count = 0
                persistence_total = 0
                
                for test_key, test_result in persist_results.items():
                    if isinstance(test_result, dict) and "success" in test_result:
                        persistence_total += 1
                        if test_result.get("success", False) and test_result.get("data_integrity", False):
                            persistence_success_count += 1
                
                if persistence_total > 0:
                    persistence_rate = persistence_success_count / persistence_total
                    metrics["test_details"]["data_persistence_rate"] = persistence_rate
                    
                    if persistence_rate >= 0.8:
                        metrics["successful_categories"] += 1
                    else:
                        metrics["critical_failures"].append("ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã®æˆåŠŸç‡ãŒä½ã„")
            else:
                metrics["critical_failures"].append("ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ããªã„")
        
        # ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆçµæœåˆ†æ
        if "error_handling" in all_results:
            error_results = all_results["error_handling"]
            metrics["total_test_categories"] += 1
            
            if not error_results.get("error"):
                # ã‚¨ãƒ©ãƒ¼å‡¦ç†é©åˆ‡æ€§è¨ˆç®—
                error_handling_success_count = 0
                error_handling_total = 0
                
                for category_key, category_result in error_results.items():
                    if isinstance(category_result, dict) and "success" in category_result:
                        error_handling_total += 1
                        if category_result.get("success", False):
                            error_handling_success_count += 1
                
                if error_handling_total > 0:
                    error_handling_rate = error_handling_success_count / error_handling_total
                    metrics["test_details"]["error_handling_appropriateness_rate"] = error_handling_rate
                    
                    if error_handling_rate >= 0.7:
                        metrics["successful_categories"] += 1
                    else:
                        metrics["critical_failures"].append("ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®é©åˆ‡æ€§ãŒä¸ååˆ†")
            else:
                metrics["critical_failures"].append("ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ããªã„")
        
        # å…¨ä½“æˆåŠŸç‡è¨ˆç®—
        if metrics["total_test_categories"] > 0:
            metrics["overall_success_rate"] = metrics["successful_categories"] / metrics["total_test_categories"]
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if metrics["overall_success_rate"] >= 0.9:
            metrics["recommendations"] = [
                "æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ã®æº–å‚™",
                "ç¶™ç¶šçš„ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®š",
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ¤œè¨"
            ]
        elif metrics["overall_success_rate"] >= 0.7:
            metrics["recommendations"] = [
                "å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è©³ç´°èª¿æŸ»",
                "éƒ¨åˆ†çš„ãªæ©Ÿèƒ½æ”¹å–„",
                "è¿½åŠ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"
            ]
        else:
            metrics["recommendations"] = [
                "é‡å¤§ãªå•é¡Œã®ä¿®æ­£",
                "ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®è¦‹ç›´ã—",
                "åŸºæœ¬æ©Ÿèƒ½ã®å†å®Ÿè£…"
            ]
        
        return metrics
    
    def print_final_summary(self, all_results: Dict[str, Any], metrics: Dict[str, Any]):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*100)
        
        # å®Ÿè¡Œæ™‚é–“è¡¨ç¤º
        if self.start_time and self.end_time:
            duration = self.end_time - self.start_time
            logger.info(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’")
        
        # å„ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªã®çµæœ
        logger.info("\nğŸ“‹ ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ:")
        
        test_categories = [
            ("åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆ", "comprehensive_integration"),
            ("ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆ", "data_persistence"),
            ("ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ", "error_handling")
        ]
        
        for category_name, category_key in test_categories:
            if category_key in all_results:
                result = all_results[category_key]
                if result.get("error"):
                    status = f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}"
                else:
                    status = "âœ… å®Ÿè¡Œå®Œäº†"
                
                logger.info(f"   {category_name}: {status}")
        
        # æˆåŠŸæŒ‡æ¨™è¡¨ç¤º
        logger.info(f"\nğŸ¯ å…¨ä½“æˆåŠŸç‡: {metrics['overall_success_rate']:.1%}")
        logger.info(f"ğŸ“ˆ æˆåŠŸã‚«ãƒ†ã‚´ãƒª: {metrics['successful_categories']:.1f}/{metrics['total_test_categories']}")
        
        # è©³ç´°æŒ‡æ¨™
        if metrics["test_details"]:
            logger.info("\nğŸ“Š è©³ç´°æŒ‡æ¨™:")
            for metric_name, metric_value in metrics["test_details"].items():
                if isinstance(metric_value, float):
                    logger.info(f"   {metric_name}: {metric_value:.1%}")
                else:
                    logger.info(f"   {metric_name}: {metric_value}")
        
        # é‡å¤§ãªå•é¡Œ
        if metrics["critical_failures"]:
            logger.info("\nâš ï¸  é‡å¤§ãªå•é¡Œ:")
            for failure in metrics["critical_failures"]:
                logger.info(f"   - {failure}")
        
        # ç·åˆè©•ä¾¡
        logger.info("\n" + "="*100)
        
        if metrics["overall_success_rate"] >= 0.9:
            logger.info("ğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ å¤§æˆåŠŸï¼")
            logger.info("âœ… ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãŒé«˜å“è³ªã§å‹•ä½œã—ã¦ã„ã¾ã™")
            logger.info("ğŸš€ æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ")
        elif metrics["overall_success_rate"] >= 0.7:
            logger.info("âœ… çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ æˆåŠŸ")
            logger.info("âš ï¸  ä¸€éƒ¨æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ãŒã€åŸºæœ¬æ©Ÿèƒ½ã¯æ­£å¸¸ã§ã™")
            logger.info("ğŸ”§ æŒ‡æ‘˜ã•ã‚ŒãŸå•é¡Œã‚’ä¿®æ­£å¾Œã€æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        elif metrics["overall_success_rate"] >= 0.5:
            logger.info("âš ï¸  çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ éƒ¨åˆ†æˆåŠŸ")
            logger.info("ğŸ”§ é‡è¦ãªå•é¡ŒãŒã„ãã¤ã‹ç™ºè¦‹ã•ã‚Œã¾ã—ãŸ")
            logger.info("ğŸ› ï¸  å•é¡Œä¿®æ­£å¾Œã«å†ãƒ†ã‚¹ãƒˆãŒå¿…è¦ã§ã™")
        else:
            logger.error("âŒ çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ å¤±æ•—")
            logger.error("ğŸš¨ ã‚·ã‚¹ãƒ†ãƒ ã«é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™")
            logger.error("ğŸ› ï¸  åŸºæœ¬æ©Ÿèƒ½ã‹ã‚‰è¦‹ç›´ã—ãŒå¿…è¦ã§ã™")
        
        # æ¨å¥¨äº‹é …
        logger.info("\nğŸ’¡ æ¨å¥¨äº‹é …:")
        for recommendation in metrics["recommendations"]:
            logger.info(f"   - {recommendation}")
        
        logger.info("\nğŸ“„ è©³ç´°ãªãƒ†ã‚¹ãƒˆçµæœã¯å„ãƒ†ã‚¹ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ¯ çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")
        logger.info("ã‚¿ã‚¹ã‚¯27.2: çµ±åˆãƒ†ã‚¹ãƒˆã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ")
        logger.info("="*100)
        
        self.start_time = time.time()
        
        # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if not await self.check_prerequisites():
            return {"error": "Prerequisites not met"}
        
        all_results = {}
        
        try:
            # 1. åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
            logger.info("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—1: åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            all_results["comprehensive_integration"] = await self.run_comprehensive_integration_tests()
            
            # 2. ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
            logger.info("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            all_results["data_persistence"] = await self.run_data_persistence_tests()
            
            # 3. ã‚¨ãƒ©ãƒ¼å‡¦ç†é©åˆ‡æ€§ç¢ºèªãƒ†ã‚¹ãƒˆ
            logger.info("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¨ãƒ©ãƒ¼å‡¦ç†é©åˆ‡æ€§ç¢ºèªãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            all_results["error_handling"] = await self.run_error_handling_tests()
            
            self.end_time = time.time()
            
            # å…¨ä½“çš„ãªæˆåŠŸæŒ‡æ¨™è¨ˆç®—
            metrics = self.calculate_overall_success_metrics(all_results)
            
            # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self.print_final_summary(all_results, metrics)
            
            # çµæœä¿å­˜
            await self.save_comprehensive_results(all_results, metrics)
            
            return {
                "test_results": all_results,
                "metrics": metrics,
                "success": metrics["overall_success_rate"] >= 0.7
            }
            
        except Exception as e:
            self.end_time = time.time()
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return {"error": str(e)}
    
    async def save_comprehensive_results(self, all_results: Dict[str, Any], metrics: Dict[str, Any]):
        """åŒ…æ‹¬çš„çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"integration_e2e_comprehensive_results_{timestamp}.json"
        
        comprehensive_data = {
            "timestamp": timestamp,
            "test_type": "integration_e2e_comprehensive",
            "task": "27.2 çµ±åˆãƒ†ã‚¹ãƒˆã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ",
            "execution_time": {
                "start_time": self.start_time,
                "end_time": self.end_time,
                "duration_seconds": self.end_time - self.start_time if self.start_time and self.end_time else 0
            },
            "test_results": all_results,
            "success_metrics": metrics,
            "requirements_verification": {
                "user_journey_tested": "comprehensive_integration" in all_results,
                "morning_to_evening_flow_tested": "comprehensive_integration" in all_results,
                "data_persistence_verified": "data_persistence" in all_results,
                "error_handling_verified": "error_handling" in all_results,
                "system_operation_guaranteed": metrics["overall_success_rate"] >= 0.7
            },
            "task_completion_status": "completed" if metrics["overall_success_rate"] >= 0.7 else "needs_improvement"
        }
        
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(comprehensive_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ“„ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {result_file}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = IntegrationE2ETestRunner()
    
    try:
        results = await runner.run_all_tests()
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
        if results.get("success", False):
            logger.info("âœ… çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Œäº†")
            sys.exit(0)
        else:
            logger.error("âŒ çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã§å•é¡ŒãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)
    except Exception as e:
        logger.error(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())